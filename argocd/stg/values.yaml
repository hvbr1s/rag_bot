env: stg
datadog: true
image:
  name: "ghcr.io/ledgerhq/knowledge_bot"
  pullSecret: "github-ledgerhq"
  tag: "v1.68.0"

hpa:
  enabled: true
  maxReplicas: 2
  minReplicas: 1
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 75

host: "knowlbot.aws.stg.ldg-tech.com"
# MO-8812:
# That's required as public integration with Slack services,
# which are also in the cloud and don't have static IPs.
whitelistSourceRange: "0.0.0.0/0,::/0"
whitelistSourceRangeRestricted: "52.50.31.80/32,34.240.150.212/32,213.215.6.86/32,89.3.198.165/32,62.23.155.50/32,194.79.179.82/32,18.168.172.238/32,54.75.130.200/32,52.213.55.186/32,52.31.65.113/32,34.250.176.119/32,18.203.166.182/32,54.75.68.198/32,37.58.221.152/32,77.158.138.66/32,37.58.221.154/32,77.158.138.67/32,0.0.0.0/0,::/0"
serverAlias:
  - knowlbot.staging.aws.ledger.fr
#serverSnippet:
#  nginx.ingress.kubernetes.io/server-snippet: |
#    set $X-Forwarded-For $X-Original-Forwarded-For;
#    rewrite foo bar permanent;
port: 80
livenessProbePath: "/_health"
readinessProbePath: "/_health"

environment:
  - name: DD_LOGS_INJECTION
    value: "true"
  - name: DD_PROFILING_ENABLED
    value: "false"
  - name: DD_JMXFETCH_ENABLED
    value: "true"
  - name: DD_RUNTIME_METRICS_ENABLED
    value: "true"
  - name: DD_TRACE_AGENT_PORT
    value: "8126"

resources:
  requests:
    memory: "400Mi"
    cpu: "250m"
  limits:
    memory: "2Gi"
    cpu: "750m"

external-secrets:
  tf-openai:
    dataFrom:
      - knowledge-bot-stg/tf-openai
  tf-alchemy:
    dataFrom:
      - knowledge-bot-stg/tf-alchemy
  tf-pinecone:
    dataFrom:
      - knowledge-bot-stg/tf-pinecone
  env-file:
    data:
      - key: knowledge-bot-stg/env-file
        name: .env
  github-ledgerhq:
    data:
      - key: infra/dockerconfigjson-github-com/v01
        name: .dockerconfigjson
    template:
      type: kubernetes.io/dockerconfigjson
